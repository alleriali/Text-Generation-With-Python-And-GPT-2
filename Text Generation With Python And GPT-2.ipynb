{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "anonymous-effects",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "blessed-baker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f26ca02af14f619bf759cf935797ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a50bce272754e629d2aec5e2795cf06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d63675194d03462e8713d8e2b58d63a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d326e82204bb4857bfbb4359fb04e134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e2e878502d24f4e84b5516eedcf713a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/523M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize tokenizer and model from pretrained GPT2 model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-hundred",
   "metadata": {},
   "source": [
    "<b>Tokenization</b>\n",
    "\n",
    "The tokenizer is used to translate between human-readable text and numeric indices. These indices are then mapped to word embeddings (numerical representations of words) by an embedding layer within the model.\n",
    "\n",
    "All we need to do to tokenizer our input text is call the tokenizer.encode method like so:\n",
    "<b>inputs = tokenizer.encode(sequence, return_tensors='pt')</b>\n",
    "\n",
    "<i>Because we are using PyTorch, we add return_tensor='pt', if using TensorFlow, we would use return_tensor='tf'</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-financing",
   "metadata": {},
   "source": [
    "<b>Generate</b>\n",
    "\n",
    "Now that we have our tokenization input text, we can begin generating text with GPT-2! All we do is call the model.generate method:\n",
    "we pass a maximum output length of 200 tokens\n",
    "<b>outputs = model.generate(inputs, max_length=200, do_sample=True)</b>\n",
    "Here we set the maximum number of tokens to generate as 200. We also add <b>do_sample=True</b> to stop the model from just picking the most likely word at every step, which ends up looking like this:\n",
    "\n",
    "He began his premiership by forming a five-man war cabinet which included Chamerlain as Lord President of the Council, Labour leader Clement Attlee as Lord Privy Seal (later as Deputy Prime Minister), Halifax as Foreign Secretary and Labour's Arthur Greenwood as a minister without portfolio. In practice,<b> he was a very conservative figure, but he was also a very conservative figure in the sense that he was a very conservative figure in the sense that he was a very conservative figure in the sense that he was a very conservative figure in the sense that he was a very conservative figure in the sense that he was a very conservative figure in the sense that he was a very conservative figure in the sense that he was a very conservative figure</b>\n",
    "\n",
    "The <b>top_k</b> and <b>temperature</b> arguments can be used to modify our outputs' coherence/randomness — we will cover these at the end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-guitar",
   "metadata": {},
   "source": [
    "<b>Decoding</b>\n",
    "\n",
    "Our generate step outputs an array of tokens rather than words. To convert these tokens into words, we need to <b>.decode</b> them. This is easy to do:\n",
    "<b>text = tokenizer.decode(outputs[0], skip_special_tokens=True)</b>\n",
    "All we need to add is <b>skip_special_tokens=True</b> to avoid decoding special tokens that are used by the model, such as the end of sequence token <|endoftext|>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-value",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    inputs, max_length=200, do_sample=True\n",
    ")\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-article",
   "metadata": {},
   "source": [
    "We can add more randomness with <b>temperature</b> — the default value is 1, a high value like 5 will produce a pretty nonsensical output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-reserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    inputs, max_length=200, do_sample=True, temperature=5\n",
    ")\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-producer",
   "metadata": {},
   "source": [
    "<b>Turning the temperature down below 1 </b> will produce more linear but less creative outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-mandate",
   "metadata": {},
   "source": [
    "We can also add the <b>top_k</b> parameter — which limits the sample tokens to a given number of the most probable tokens. This results in text that tends to stick to the same topic (or set of words) for a longer period of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-south",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
